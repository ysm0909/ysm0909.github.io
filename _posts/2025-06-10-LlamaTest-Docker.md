---
layout: post
title:  "Llama Test 2(GGUF+Docker)"
date: 2025-06-10 11:13:36 +0900
categories: Llama
---

reference: [A Quick Guide to Containerizing Llamafile with Docker for AI Applications (Sophia Parafina)][link]

## **1. To get started, copy, paste, and save the following in a file named Dockerfile.**

Debian Trixieë¥¼ ê¸°ë°˜ìœ¼ë¡œ Mozilla-Ochoì˜ [llamafile](https://github.com/Mozilla-Ocho/llamafile) í”„ë¡œì íŠ¸ë¥¼ ë¹Œë“œí•˜ê³ , ê°€ë²¼ìš´ ì´ë¯¸ì§€ë¡œ íŒ¨í‚¤ì§•í•˜ì—¬ ì‹¤í–‰ ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆë¥¼ êµ¬ì„±í•˜ëŠ” ê²ƒ

```docker
# < 1ë‹¨ê³„: ë¹Œë“œ ìŠ¤í…Œì´ì§€ë¡œ Debian Trixieë¥¼ ì‚¬ìš© (GCC 13 ì‚¬ìš© ê°€ëŠ¥) >
# Use debian trixie for gcc13
FROM debian:trixie as builder
 
# Set work directory
WORKDIR /download
 
# Configure build container and build llamafile
RUN mkdir out && \
    apt-get update && \
    apt-get install -y curl git gcc make && \ # í•„ìš”í•œ ë„êµ¬ ì„¤ì¹˜
    git clone https://github.com/Mozilla-Ocho/llamafile.git  && \ # ì†ŒìŠ¤ ì½”ë“œ í´ë¡ 
    curl -L -o ./unzip https://cosmo.zip/pub/cosmos/bin/unzip && \ # ì»¤ìŠ¤í…€ unzip ë„êµ¬ ë‹¤ìš´ë¡œë“œ
    chmod 755 unzip && mv unzip /usr/local/bin && \ # ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬ ë° ê²½ë¡œ ì´ë™
    cd llamafile && make -j8 LLAMA_DISABLE_LOGS=1 && \ # ë¡œê¹… ë¹„í™œì„±í™”í•˜ê³  ë³‘ë ¬ ë¹Œë“œ ìˆ˜í–‰
    make install PREFIX=/download/out # ë¹Œë“œ ê²°ê³¼ë¬¼ì„ ì§€ì •ëœ ê²½ë¡œë¡œ ì„¤ì¹˜
 
# < 2ë‹¨ê³„: ì‹¤í–‰ ìŠ¤í…Œì´ì§€ë¡œ Debian stable ì‚¬ìš© (ë” ì‘ê³  ì•ˆì „í•œ ì´ë¯¸ì§€) >
# Create container
FROM debian:stable as out
 
# Create a non-root user
RUN addgroup --gid 1000 user && \
    adduser --uid 1000 --gid 1000 --disabled-password --gecos "" user
 
# Switch to user
USER user
 
# Set working directory
WORKDIR /usr/local
 
# Copy llamafile and man pages
COPY --from=builder /download/out/bin ./bin
COPY --from=builder /download/out/share ./share/man
 
# Expose 8080 port.
EXPOSE 8080
 
# Set entrypoint.
ENTRYPOINT ["/bin/sh", "/usr/local/bin/llamafile"]
 
# Set default command.
CMD ["--server", "--host", "0.0.0.0", "-m", "/model"]
```

## **2. To build the container, run:**

```bash
docker build -t llamafile .
```

â›” **Error 1**

![image.png](/assets/image/2025-06-10-LlamaTest-Docker-Image/image.png)

**ğŸ’¡ Solution**

ì°¸ê³ : [https://seulcode.tistory.com/557](https://seulcode.tistory.com/557)

1. docker group ìƒì„±
    
    ```bash
    sudo groupadd docker
    ```
    
2. docker groupì— ìœ ì € ì¶”ê°€
    
    ```bash
    sudo usermod -aG docker $USER
    ```
    
3. ì•„ë˜ ëª…ë ¹ì–´ ì‹¤í–‰
    
    ```bash
    newgrp docker
    ```
    

â›” **Error 2**

![image.png](/assets/image/2025-06-10-LlamaTest-Docker-Image/image%201.png)

**ğŸ’¡ Solution**

```docker
# Configure build container and build llamafile
RUN mkdir out && \
    apt-get update && \
    apt-get install -y curl git gcc make && \ # í•„ìš”í•œ ë„êµ¬ ì„¤ì¹˜
    git clone https://github.com/Mozilla-Ocho/llamafile.git  && \ # ì†ŒìŠ¤ ì½”ë“œ í´ë¡ 
    curl -L -o ./unzip https://cosmo.zip/pub/cosmos/bin/unzip && \ # ì»¤ìŠ¤í…€ unzip ë„êµ¬ ë‹¤ìš´ë¡œë“œ
    chmod 755 unzip && mv unzip /usr/local/bin && \ # ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬ ë° ê²½ë¡œ ì´ë™
    cd llamafile && make -j8 LLAMA_DISABLE_LOGS=1 && \ # ë¡œê¹… ë¹„í™œì„±í™”í•˜ê³  ë³‘ë ¬ ë¹Œë“œ ìˆ˜í–‰
    make install PREFIX=/download/out # ë¹Œë“œ ê²°ê³¼ë¬¼ì„ ì§€ì •ëœ ê²½ë¡œë¡œ ì„¤ì¹˜
```

 **ì—¬ëŸ¬ ê°œì˜ RUN ëª…ë ¹ì–´ë¡œ ë‚˜ëˆ”**

```docker
# Prepare output directory
RUN mkdir /download/out
# Install required packages
RUN apt-get update && \
    apt-get install -y curl git gcc g++ make build-essential zlib1g-dev
# Clone llamafile repository
RUN git clone https://github.com/Mozilla-Ocho/llamafile.git
# Install unzip from cosmos.zip
RUN curl -L -o ./unzip https://cosmo.zip/pub/cosmos/bin/unzip && \
    chmod 755 unzip && mv unzip /usr/local/bin
# Set working directory to the repo
WORKDIR /download/llamafile
# Build llamafile
RUN make LLAMA_DISABLE_LOGS=1
# Install llamafile to output dir
RUN make install PREFIX=/download/out
```

## **3. LLamafile(GGUF) Download**

[(Huggingface) Llama-3.3-70B-Instruct-GGUF ][link2]

## **4. Running the llamafile container**

```bash
docker run -d -v ./model/<ëª¨ë¸ëª….gguf>:/model -p 8080:8080 llamafile
```

â¡ï¸ model í´ë” ì•ˆì— ìˆì–´ì•¼ í•¨.

![image.png](/assets/image/2025-06-10-LlamaTest-Docker-Image/image%202.png)

```
http://127.0.0.1:8080
```

â¡ï¸ llama.cpp interface ì ‘ê·¼

![image.png](/assets/image/2025-06-10-LlamaTest-Docker-Image/image%203.png)

# 5. Test

âœ… **Test 1**

![image.png](/assets/image/2025-06-10-LlamaTest-Docker-Image/image%204.png)

**âœ… Test2**

```bash
curl -s http://localhost:8080/v1/chat/completions -H "Content-Type: application/json" -d '{
  "model": "gpt-3.5-turbo",
  "messages": [
    {
      "role": "system",
      "content": "You are a poetic assistant, skilled in explaining complex programming concepts with creative flair."
    },
    {
      "role": "user",
      "content": "Compose a poem that explains the concept of recursion in programming."
    }
  ]
}' | python3 -c '
import json
import sys
json.dump(json.load(sys.stdin), sys.stdout, indent=2)
print()
'
```

![image.png](/assets/image/2025-06-10-LlamaTest-Docker-Image/image%205.png)

**â¬‡ï¸ output**

![image.png](/assets/image/2025-06-10-LlamaTest-Docker-Image/image%206.png)

âœ… **Check**

- ì •ìƒ ì‘ë™ ì—¬ë¶€ í™•ì¸

```bash
docker ps
```

![image.png](/assets/image/2025-06-10-LlamaTest-Docker-Image/image%207.png)

- ì¢…ë£Œëœ ì»¨í…Œì´ë„ˆ í™•ì¸

```bash
docker ps -a
```

![image.png](/assets/image/2025-06-10-LlamaTest-Docker-Image/image%208.png)

- ë¡œê·¸ í™•ì¸

```bash
docker logs <ì»¨í…Œì´ë„ˆ ID>
```

â¡ï¸ ë¡œê·¸í†µí•´ ì‚¬ì´íŠ¸ ì ‘ì† llama.cpp interface ì ‘ê·¼ ê°€ëŠ¥

![image.png](/assets/image/2025-06-10-LlamaTest-Docker-Image/image%209.png)

- ì»¨í…Œì´ë„ˆ ì¢…ë£Œ

```bash
docker stop <ì»¨í…Œì´ë„ˆ ID>
```


[link]: https://www.docker.com/blog/a-quick-guide-to-containerizing-llamafile-with-docker-for-ai-applications/
[link2]: https://huggingface.co/bartowski/Llama-3.3-70B-Instruct-GGUF/blob/main/Llama-3.3-70B-Instruct-Q3_K_M.gguf

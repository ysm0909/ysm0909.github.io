---
layout: post
title:  "Llama Fine-tuning Test(Unsloth)"
date: 2025-06-19 13:13:36 +0900
categories: Llama
---

reference: [Unsloth][link]

# **âš™ï¸** Setting

## âœ… 1. ê°€ìƒí™˜ê²½ ìƒì„±**â—**

```bash
#step1
conda create -n unsloth_env python=3.11 -y

#step2
conda activate unsloth_env

#step3
# PyTorch + CUDA 12.1 (pipë¡œ ì„¤ì¹˜, ê³µì‹ wheel)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

## âœ… 2. CUDA ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸

```bash
nvcc --version
```

![image.png](/assets/image/2025-06-19-Llama-Fine-tuning-Test-Unsloth-Image/image.png)

## âœ… 3. PyTorchê°€ GPUë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ í™•ì¸

```python
import torch
print("CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€: ", torch.cuda.is_available())
print("GPU: ", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "ì‚¬ìš©ë¶ˆê°€")
```

![image.png](/assets/image/2025-06-19-Llama-Fine-tuning-Test-Unsloth-Image/image%201.png)

## âœ… 4. PyTorch GPU ë²„ì „ ì„¤ì¹˜ í™•ì¸

![image.png](/assets/image/2025-06-19-Llama-Fine-tuning-Test-Unsloth-Image/image%202.png)

âš ï¸ í˜¸í™˜ ë²„ì „ í™•ì¸ â¬‡ï¸

For other torch versions, we supportÂ `torch211`,Â `torch212`,Â `torch220`,Â `torch230`,Â `torch240`Â and for CUDA versions, we supportÂ `cu118`Â andÂ `cu121`Â andÂ `cu124`.Â 

## âš ï¸ VS Code ê²½ê³  í•´ê²°

```
We noticed you're using a conda environment. If you are experiencing issues with this environment in the integrated terminal, we recommend that you let the Python extension change "terminal.integrated.inheritEnv" to false in your user settings
```

â¡ï¸ VS Codeì—ì„œ ì—´ë ¤ ìˆëŠ” í„°ë¯¸ë„ì´ **conda ê°€ìƒí™˜ê²½ì„ ì œëŒ€ë¡œ ì¸ì‹í•˜ì§€ ëª»í•  ìˆ˜ë„ ìˆë‹¤**ëŠ” ì˜ë¯¸

- `Ctrl + Shift + P` â†’ `Preferences: Open User Settings (JSON)`

![image.png](/assets/image/2025-06-19-Llama-Fine-tuning-Test-Unsloth-Image/image%203.png)

# 1ï¸âƒ£ Installation

```bash
pip install unsloth
```

â•python í™˜ê²½: 3.10 ì´ìƒ

![image.png](/assets/image/2025-06-19-Llama-Fine-tuning-Test-Unsloth-Image/image%204.png)

â¡ï¸ torchaudio ì‚¬ìš©í•  ì¼ ì—†ì„ ê±° ê°™ìœ¼ë‹ˆ ì¼ë‹¨ ë³´ë¥˜

# 2ï¸âƒ£ **Unsloth**

## â¡ï¸ `unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit`  ì‚¬ìš©!

from datasets import load_dataset
dataset = load_dataset("hpe-ai/medical-cases-classification-tutorial", split="train")

```python
# âœ… Unslothì—ì„œ ì œê³µí•˜ëŠ” ë¹ ë¥¸ LLM ë¡œë”© ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ import
from unsloth import FastLanguageModel

# âœ… PyTorchëŠ” í…ì„œ ê³„ì‚°, GPU ì—°ì‚° ë“±ì„ ìœ„í•œ í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
import torch

# âœ… ìµœëŒ€ í† í° ê¸¸ì´ ì„¤ì • (ì˜ˆ: í•˜ë‚˜ì˜ ë¬¸ì¥ì´ 2048 í† í°ê¹Œì§€ ê°€ëŠ¥)
max_seq_length = 2048  # ì›í•˜ëŠ” ê¸¸ì´ë¡œ ì„¤ì • ê°€ëŠ¥! ë‚´ë¶€ì ìœ¼ë¡œ RoPE Scaling ì§€ì›ë¨

# âœ… ë°ì´í„° íƒ€ì… ì„¤ì •
# - Noneìœ¼ë¡œ ë‘ë©´ ìë™ ê°ì§€ë¨
# - float16ì€ T4, V100 GPUì— ì í•©
# - bfloat16ì€ A100, RTX 3090, 4090 ë“± Ampere ì´ìƒì— ì í•©
dtype = None

# âœ… 4bit ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€
# - Trueë¡œ í•˜ë©´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì¤„ê³  í•™ìŠµ ë° ì¶”ë¡ ì´ ë” ë¹¨ë¼ì§
load_in_4bit = True

# âœ… ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",  # ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
    max_seq_length = max_seq_length,          # ìµœëŒ€ ì…ë ¥ ê¸¸ì´
    dtype = dtype,                            # float16, bfloat16, None ì¤‘ ì„ íƒ
    load_in_4bit = load_in_4bit,              # 4bit ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€(ì–‘ìí™” ëª¨ë¸: ë‹¤ì‹œ ì–‘ìí™”í•˜ëŠ” ê²Œ ì•„ë‹ˆë¼, ì˜¬ë°”ë¥´ê²Œ ë¡œë”©í•˜ëŠ” ìš©ë„)
    token = "hf_..."                        # í—ˆê¹…í˜ì´ìŠ¤ì˜ ì ‘ê·¼ í† í° (Gated ëª¨ë¸ì¸ ê²½ìš° í•„ìš”)
)
```

![image.png](/assets/image/2025-06-19-Llama-Fine-tuning-Test-Unsloth-Image/image%205.png)

```
# âœ… ë¯¸ë¦¬ 4bitë¡œ ì–‘ìí™”ëœ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
# - ë‹¤ìš´ë¡œë“œ ì†ë„ê°€ ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ ë¶€ì¡±(OOM) ë¬¸ì œë„ ì¤„ì–´ë“¦
fourbit_models = [
    "unsloth/Meta-Llama-3.1-8B-bnb-4bit",      # Llama 3.1 - 8B ëª¨ë¸, 2ë°° ë¹ ë¥¸ ì„±ëŠ¥
    "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
    "unsloth/Meta-Llama-3.1-70B-bnb-4bit",
    "unsloth/Meta-Llama-3.1-405B-bnb-4bit",    # 405B ëª¨ë¸ë„ 4bitë¡œ ì§€ì›!
    "unsloth/Mistral-Nemo-Base-2407-bnb-4bit", # ìƒˆ ë²„ì „ Mistral 12B ëª¨ë¸, ì„±ëŠ¥ í–¥ìƒ
    "unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit",
    "unsloth/mistral-7b-v0.3-bnb-4bit",        # Mistral v0.3 ë²„ì „
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/Phi-3.5-mini-instruct",           # Phi 3.5 ëª¨ë¸
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/gemma-2-9b-bnb-4bit",
    "unsloth/gemma-2-27b-bnb-4bit",            # Google Gemma ëª¨ë¸ë„ ì§€ì›ë¨
]  # ì „ì²´ ëª¨ë¸ ëª©ë¡ì€ https://huggingface.co/unsloth ì°¸ê³ 
```

### ğŸ“ `FastLanguageModel.from_pretrained`

- Hugging Faceì— ìˆëŠ” ëª¨ë¸ì„ **ë¹ ë¥´ê³  íš¨ìœ¨ì ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•œ í•¨ìˆ˜**
- Unslothê°€ ë‚´ë¶€ì ìœ¼ë¡œ 4bit ì§€ì›, í† í¬ë‚˜ì´ì € ìµœì í™”, GPU ìµœì í™” ë“± ì²˜ë¦¬

```python
# LoRA adapters 
model = FastLanguageModel.get_peft_model(
    model,
    
    # LoRAì˜ rank ê°’. í´ìˆ˜ë¡ í‘œí˜„ë ¥ì€ ì˜¬ë¼ê°€ì§€ë§Œ VRAM ì‚¬ìš©ëŸ‰ë„ ì¦ê°€í•©ë‹ˆë‹¤.
    r = 16,  # 8, 16, 32, 64, 128 ë“± ì‚¬ìš© ê°€ëŠ¥

    # LoRAë¥¼ ì ìš©í•  ëŒ€ìƒ ëª¨ë“ˆë“¤. Transformerì˜ í•µì‹¬ ì—°ì‚° ë¶€ë¶„ì…ë‹ˆë‹¤.
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],

    # LoRAì˜ scaling factorë¡œ, ë³´í†µ rê³¼ ê°™ì€ ê°’ì„ ì£¼ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.
    lora_alpha = 16,

    # LoRAì— ì ìš©í•  dropout ë¹„ìœ¨. 0ìœ¼ë¡œ ì„¤ì •í•˜ë©´ ì„±ëŠ¥ ë° ì†ë„ ìµœì í™”ì— ìœ ë¦¬í•©ë‹ˆë‹¤.
    lora_dropout = 0,  # ì–´ë–¤ ê°’ì´ë“  ê°€ëŠ¥í•˜ë‚˜, 0ì´ ê°€ì¥ ìµœì í™”ë˜ì–´ ìˆìŒ

    # bias í•™ìŠµ ì—¬ë¶€. "none"ì´ë©´ bias íŒŒë¼ë¯¸í„°ëŠ” í•™ìŠµí•˜ì§€ ì•ŠìŒ (ì„±ëŠ¥/ì†ë„ ìµœì í™”ë¨)
    # bias: ë‰´ëŸ°ì´ ë” ë‹¤ì–‘í•œ ê°’ì„ ì¶œë ¥í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ì¶”ê°€ì ì¸ ìƒìˆ˜
		# LoRAì—ì„œëŠ” ëŒ€ë¶€ë¶„ í•™ìŠµ ëŒ€ìƒì—ì„œ ì œì™¸ì‹œì¼œë„ ë¬´ë°©
    bias = "none",

    # âœ… "unsloth"ì˜ ì»¤ìŠ¤í…€ ì²´í¬í¬ì¸íŒ… ë°©ì‹ì€ VRAMì„ 30% ì ˆì•½í•˜ê³ , batch sizeë¥¼ 2ë°° í‚¤ìš¸ ìˆ˜ ìˆìŒ
    use_gradient_checkpointing = "unsloth",  # True ë˜ëŠ” "unsloth" ì‚¬ìš© ê°€ëŠ¥

    # ëœë¤ì„± ê³ ì •ì„ ìœ„í•œ ì‹œë“œê°’ ì„¤ì • (ì¬í˜„ì„± ë³´ì¥)
    random_state = 3407,

    # LoRAì˜ ë³€í˜• ê¸°ë²• ì¤‘ í•˜ë‚˜ì¸ Rank-Stabilized LoRA ì‚¬ìš© ì—¬ë¶€
    use_rslora = False,  # ì¼ë°˜ì ì¸ LoRA ì‚¬ìš©

    # LoftQë¼ëŠ” ë˜ ë‹¤ë¥¸ ì–‘ìí™” í•™ìŠµ ê¸°ë²• ì„¤ì • (ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ Noneìœ¼ë¡œ ì„¤ì •)
    loftq_config = None,
)
```

# 3ï¸âƒ£ Data Prep

```python
# Alpaca í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

# ëª¨ë¸ì˜ EOS (End of Sequence) í† í°ì„ ê°€ì ¸ì˜´ â€“ í…ìŠ¤íŠ¸ ë í‘œì‹œìš©
EOS_TOKEN = tokenizer.eos_token  # ë°˜ë“œì‹œ EOS_TOKENì„ ì¶”ê°€í•´ì•¼ í•¨ (í…ìŠ¤íŠ¸ê°€ ë¬´í•œíˆ ìƒì„±ë˜ì§€ ì•Šë„ë¡ í•¨)

# ë°ì´í„°ì…‹ì˜ ê° ì˜ˆì œë¥¼ Alpaca í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ ì •ì˜
def formatting_prompts_func(examples):
    # ë¶„ë¥˜ ëª¨ë¸ ìƒì„±ìœ„í•´ Instruction ì •ì˜
    instructions = "Classify the following medical transcription into the correct medical specialty."
    inputs       = examples["input"]        # input ì—´ ê°€ì ¸ì˜¤ê¸°
    outputs      = examples["output"]       # output ì—´ ê°€ì ¸ì˜¤ê¸°
    
    # instruction, input, outputì„ í”„ë¡¬í”„íŠ¸ í˜•ì‹ì— ë§ê²Œ ì±„ìš°ê³  EOS í† í° ì¶”ê°€
    texts = [
        alpaca_prompt.format(instruction, input_, output_) + EOS_TOKEN
        for input_, output_ in zip(inputs, outputs)
    ]
    return {"text": texts} # HuggingFace datasets í¬ë§·ì— ë§ê²Œ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜
# ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸° (train splitë§Œ ì‚¬ìš©)
from datasets import load_dataset
dataset = load_dataset("hpe-ai/medical-cases-classification-tutorial", split="train")

# map í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ë°ì´í„°ì…‹ ì „ì²´ì— formatting_prompts_func ì ìš©
# batched=True ì„¤ì • ì‹œ í•œ ë²ˆì— ì—¬ëŸ¬ ìƒ˜í”Œì„ ì²˜ë¦¬ ê°€ëŠ¥ (ì„±ëŠ¥ í–¥ìƒ)
dataset = dataset.map(formatting_prompts_func, batched = True)

```

# 4ï¸âƒ£ **Train the model**

```python
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported

trainer = SFTTrainer(
    model = model,                          # íŒŒì¸íŠœë‹í•  ì‚¬ì „í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸
    tokenizer = tokenizer,                  # ëª¨ë¸ì— ë§ëŠ” í† í¬ë‚˜ì´ì €
    train_dataset = dataset,                # í•™ìŠµì— ì‚¬ìš©í•  ë°ì´í„°ì…‹
    dataset_text_field = "text",            # ë°ì´í„°ì…‹ ë‚´ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” í•„ë“œ ì´ë¦„
    max_seq_length = max_seq_length,       # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (í† í° ìˆ˜)
    dataset_num_proc = 2,                   # ë°ì´í„° ì „ì²˜ë¦¬ì— ì‚¬ìš©í•  í”„ë¡œì„¸ìŠ¤ ê°œìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬)
    packing = False,                        # ì§§ì€ ì‹œí€€ìŠ¤ ë¶™ì—¬ì„œ ë°°ì¹˜ ì²˜ë¦¬í•˜ëŠ” ê¸°ëŠ¥ (ì†ë„ í–¥ìƒìš©), ì—¬ê¸°ì„  ë¹„í™œì„±í™”
    args = TrainingArguments(
        per_device_train_batch_size = 8,   # í•œ GPU ë‹¹ ë°°ì¹˜ í¬ê¸°
        gradient_accumulation_steps = 4,   # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í… ìˆ˜ (ì‹¤ì§ˆ ë°°ì¹˜ í¬ê¸° = 2 * 4 = 8)
        warmup_steps = 5,                  # í•™ìŠµ ì´ˆê¸° í•™ìŠµë¥  ì ì§„ì  ì¦ê°€ ê¸°ê°„
        # num_train_epochs = 1,            # ì „ì²´ ì—í¬í¬ ìˆ˜ (ì£¼ì„ ì²˜ë¦¬ë¨, ëŒ€ì‹  max_steps ì‚¬ìš©)
        max_steps = 60,                    # ìµœëŒ€ í•™ìŠµ ìŠ¤í… ìˆ˜ (60 ìŠ¤í…ê¹Œì§€ë§Œ í•™ìŠµ)
        learning_rate = 2e-4,             # í•™ìŠµë¥ 
        fp16 = not is_bfloat16_supported(),   # bf16 ë¯¸ì§€ì› ì‹œ fp16 ì‚¬ìš© (í˜¼í•© ì •ë°€ë„)
        bf16 = is_bfloat16_supported(),        # bf16 ì§€ì› ì‹œ í™œì„±í™”
        logging_steps = 1,                # ëª‡ ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ ê¸°ë¡í• ì§€ (ì—¬ê¸°ì„  ë§¤ ìŠ¤í…)
        optim = "adamw_8bit",             # 8ë¹„íŠ¸ ì–‘ìí™”ëœ AdamW ì˜µí‹°ë§ˆì´ì € ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½ ëª©ì )
        weight_decay = 0.01,              # ê°€ì¤‘ì¹˜ ê°ì‡  ê³„ìˆ˜ (ì •ê·œí™”)
        lr_scheduler_type = "linear",     # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ (ì„ í˜• ê°ì†Œ)
        seed = 3407,                      # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •
        output_dir = "outputs",           # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í´ë” ê²½ë¡œ
        report_to = "none",               # WandB ë“± ì™¸ë¶€ ë¡œê·¸ ë¹„í™œì„±í™”
    ),
)

```

```python
trainer_stats = trainer.train()
```

![image.png](/assets/image/2025-06-19-Llama-Fine-tuning-Test-Unsloth-Image/image%206.png)

# 5ï¸âƒ£ **Inference**

```python
# ğŸ§¾ Prompt í˜•ì‹ ë™ì¼í•˜ê²Œ ìœ ì§€
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

# âœ… ì˜ˆì‹œ ë¬¸ì¥ (Inferenceìš©)
instruction = "Classify the following medical transcription into the correct medical specialty."
transcription = "PREOPERATIVE DIAGNOSES:,1. Right axillary adenopathy.,2. Thrombocytopenia.,3. Hepatosplenomegaly.,POSTOPERATIVE DIAGNOSES:,1. Right axillary adenopathy.,2. Thrombocytopenia.,3. Hepatosplenomegaly.,PROCEDURE PERFORMED: ,Right axillary lymph node biopsy.,ANESTHESIA: , Local with sedation.,COMPLICATIONS: , None.,DISPOSITION: , The patient tolerated the procedure well and was transferred to the recovery room in stable condition.,BRIEF HISTORY: ,The patient is a 37-year-old male who presented to ABCD General Hospital secondary to hiccups and was ultimately found to have a right axillary mass to be severely thrombocytopenic with a platelet count of 2000 as well as having hepatosplenomegaly. The working diagnosis is lymphoma, however, the Hematology and Oncology Departments were requesting a lymph node biopsy in order to confirm the diagnosis as well as prognosis. Thus, the patient was scheduled for a lymph node biopsy with platelets running secondary to thrombocytopenia at the time of surgery.,INTRAOPERATIVE FINDINGS: , The patient was found to have a large right axillary lymphadenopathy, one of the lymph node was sent down as a fresh specimen.,PROCEDURE: ,After informed written consent, risks and benefits of this procedure were explained to the patient. The patient was brought to the operating suite, prepped and draped in a normal sterile fashion. Multiple lymph nodes were palpated in the right axilla, however, the most inferior node was to be removed. First, the skin was anesthetized with 1% lidocaine solution. Next, using a #15 blade scalpel, an incision was made approximately 4 cm in length transversally in the inferior axilla. Next, using electro Bovie cautery, maintaining hemostasis, dissection was carried down to the lymph node. The lymph node was then completely excised using electro Bovie cautery as well as hemostats to maintain hemostasis and then lymph node was sent to specimen fresh to the lab. Several hemostats were used, suture ligated with #3-0 Vicryl suture and hemostasis was maintained. Next the deep dermal layers were approximated with #3-0 Vicryl suture. After the wound has been copiously irrigated, the skin was closed with running subcuticular #4-0 undyed Vicryl suture and the pathology is pending. The patient did tolerated the procedure well. Steri-Strips and sterile dressings were applied and the patient was transferred to the Recovery in stable condition."
# âœ… í”„ë¡¬í”„íŠ¸ ìƒì„± (outputì€ ë¹ˆì¹¸)
prompt = alpaca_prompt.format(instruction, transcription, "")

# âœ… í† í¬ë‚˜ì´ì¦ˆ + ì¶”ë¡ 
from transformers import AutoTokenizer

inputs = tokenizer([prompt], return_tensors="pt").to("cuda")

# âš¡ 2ë°° ì†ë„ ì˜µì…˜ (LoRA ì ìš© ëª¨ë¸ì¸ ê²½ìš°ì—ë§Œ ì‚¬ìš© ê°€ëŠ¥)
# FastLanguageModel.for_inference(model)  # PEFT ëª¨ë¸ì—ì„œëŠ” ë³´í†µ ìƒëµ ê°€ëŠ¥

outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)

# âœ… ê²°ê³¼ ë””ì½”ë”©
print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])
```

![image.png](/assets/image/2025-06-19-Llama-Fine-tuning-Test-Unsloth-Image/image%207.png)

# 6ï¸âƒ£ **Test Evaluation**

```python
from datasets import load_dataset
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import torch
from tqdm import tqdm
import numpy as np

# Alpaca prompt í…œí”Œë¦¿
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

instruction = "Classify the following medical transcription into the correct medical specialty."

# ë¼ë²¨ ì „ì²˜ë¦¬ í•¨ìˆ˜
def normalize_label(label):
    return label.lower().replace("-", "/").replace(" / ", "/").strip()

# ì…ë ¥ ê¸¸ì´ ì œí•œìš© í† í° ìˆ˜
MAX_INPUT_TOKENS = 1800

# Test dataset ë¡œë“œ
test_dataset = load_dataset("hpe-ai/medical-cases-classification-tutorial", split="test")

labels, preds = [], []

# ë¼ë²¨ ì§‘í•© í™•ë³´ (ì •ê·œí™”ëœ ê¸°ì¤€ìœ¼ë¡œ)
all_classes = sorted(list({normalize_label(x["medical_specialty"]) for x in test_dataset}))

for example in tqdm(test_dataset):
    transcription = example["transcription"]
    true_label = normalize_label(example["medical_specialty"])
    labels.append(true_label)

    prompt = alpaca_prompt.format(instruction, transcription, "")

    # í† í¬ë‚˜ì´ì§• + ê¸¸ì´ ì œí•œ
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=MAX_INPUT_TOKENS).to("cuda")

    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=32, use_cache=True)
    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # "### Response:" ì´í›„ ì˜ˆì¸¡ê°’ë§Œ ì¶”ì¶œ
    response = output_text.split("### Response:")[-1].strip().split("\n")[0]
    pred = normalize_label(response)
    preds.append(pred)

# ì •í™•ë„ ë° ë¦¬í¬íŠ¸ ì¶œë ¥
print("âœ… Accuracy:", accuracy_score(labels, preds))
print("\nğŸ“„ Classification Report:")
print(classification_report(labels, preds, labels=all_classes))

# Confusion matrix
cm = confusion_matrix(labels, preds, labels=all_classes)

plt.figure(figsize=(18, 14))
sns.heatmap(cm, xticklabels=all_classes, yticklabels=all_classes, annot=False, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()
```

![image.png](/assets/image/2025-06-19-Llama-Fine-tuning-Test-Unsloth-Image/image%208.png)

![image.png](/assets/image/2025-06-19-Llama-Fine-tuning-Test-Unsloth-Image/image%209.png)

# 7ï¸âƒ£ **Saving, loading finetuned models**

```python
model.save_pretrained("lora_model")  # Local saving
tokenizer.save_pretrained("lora_model")
# model.push_to_hub("your_name/lora_model", token = "...") # Online saving
# tokenizer.push_to_hub("your_name/lora_model", token = "...") # Online saving
```


[link]: https://github.com/unslothai/unsloth

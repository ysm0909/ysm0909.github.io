[
  
    {
      "title"    : "Llama Test 2(GGUF+Docker)",
      "title-lower"    : "llama test 2(gguf+docker)",
      "sub-titile" : "",
      "sub-titile-lower" : "",
      "categories" : "Llama",
      "tags"     : "",
      "url"      : "/llama/2025/06/10/LlamaTest-Docker.html",
      "date"     : "2025-06-10 11:13:36 +0900",
      "content"     : "reference: A Quick Guide to Containerizing Llamafile with Docker for AI Applications (Sophia Parafina)1. To get started, copy, paste, and save the following in a file named Dockerfile.Debian Trixie..."
    } ,
  
    {
      "title"    : "Llama Test 1(Meta)",
      "title-lower"    : "llama test 1(meta)",
      "sub-titile" : "",
      "sub-titile-lower" : "",
      "categories" : "Llama",
      "tags"     : "",
      "url"      : "/llama/2025/06/05/LlamaTest-Meta.html",
      "date"     : "2025-06-05 17:41:36 +0900",
      "content"     : "reference: Meta llama3.1 직접 다운로드 하기      Meta 사이트 접속    Download Llama        https://llama3-1.llamameta.net/*?Policy= ~ 생략        메일 확인 → 사이트 접속            터미널 접속    Download     pip install llama..."
    } ,
  
    {
      "title"    : "A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes",
      "title-lower"    : "a gentle introduction to 8-bit matrix multiplication for transformers at scale using hugging face transformers, accelerate and bitsandbytes",
      "sub-titile" : "",
      "sub-titile-lower" : "",
      "categories" : "HuggingFace",
      "tags"     : "",
      "url"      : "/huggingface/2025/06/04/8-bit-Matrix-Multiplication.html",
      "date"     : "2025-06-04 15:28:36 +0900",
      "content"     : "reference: Hugging Face LinkIntroductionMuch larger models, like PaLM would require even more resources.e.g. BLOOM-176B → 8x 80GB A100 GPUsSo, we need to find ways to reduce these requirements whil..."
    } 
  
]